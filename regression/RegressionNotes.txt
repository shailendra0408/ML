Regression searches for relationship among varibles. 

Regression - Scikit link - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html

1. Simple Linear Regression - Single Indipendent variable ( Training pairs or regressors) 
and single response 
    ğ‘“(ğ‘¥) = ğ‘â‚€ + ğ‘â‚ğ‘¥
    ğ‘â‚€ = interept
    ğ‘â‚ = slope 
Tried 2 examples 
    a. With 10 sample data
    b. With  around 4000 data points of T, RH. 
    Temperature is not something that varried linearly, so a linear regression model 
    will not give you good forecast, but a polynomial regression might. 


    Now the Maths behind this is from the lecture 1 of the Advance ML course. 
    The regression coefficient can be find by using 
    h bar = (X Transpose X )^-1 X Transpose. 

2. Multiple Regression - Two or More indipendent variables and single response.
    a. Use ddifferent Sources - Here is one interesting - https://medium.com/swlh/multi-linear-regression-using-python-44bd0d10082d
    b. Another one - didn't tried it - https://aegis4048.github.io/mutiple_linear_regression_and_visualization_in_python

3. Polynomial Regression 
    a. Interesting read - https://towardsdatascience.com/machine-learning-polynomial-regression-with-python-5328e4e8a386
    b. https://www.geeksforgeeks.org/python-implementation-of-polynomial-regression/
    c. Multivariate Polynomial regression model - https://data36.com/polynomial-regression-python-scikit-learn/

4. Logistic Regression 
    a. Interesting read - https://realpython.com/logistic-regression-python/
    b. Another one - https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8
    c. Smote ( Synthetic Minority Oversampling Technique )  - Its a usefull tool that is used when there is a un-balenced data. 
    More here https://towardsdatascience.com/smote-fdce2f605729#:~:text=SMOTE%20is%20a%20machine%20learning,with%20this%20type%20of%20data.
    d. Recursive Feature elimination - a feature of logistic regression 
    
        

4. Underfitting ( Low r^2 ) and Overfitting ( High R^2 with known data but low R^2 with new data)
